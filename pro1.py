# -*- coding: utf-8 -*-
"""pro1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UAPTCYMKV8iArPX2rUtyq48BNbtnSZMe
"""

from google.colab import files

# Upload the file
uploaded = files.upload()

# Access the uploaded file (assuming it's a text file)
for file_name in uploaded.keys():
    with open(file_name, 'r') as file:
        content = file.read()

from google.colab import files

# Upload the file
uploaded = files.upload()

# Access the uploaded file (assuming it's a text file)
for file_name in uploaded.keys():
    with open(file_name, 'r') as file:
        content = file.read()

# Importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

# Importing dataset and reading dataset
df = pd.read_csv('train.csv')
df.head(10)

df.shape

# Columns / Features names
df.columns

# Removing In-significant features
df1 = df.drop(['security_no','Name','referral_id','joining_date','last_visit_time','customer_id'], axis=1)
df1.head(2)

df1.info()

# Check any unknown values or special symbols for categorical features in dataset
# Replace 'Unknown' with NaN values
df1['gender'].value_counts()
df1['gender'] = df1['gender'].replace('Unknown', np.nan)

# In this column no unknown or special symbols are found
df1['region_category'].value_counts()

# Check any unknown values or special symbols for categorical features in dataset
df1['membership_category'].value_counts()

# Check any unknown values or special symbols for categorical features in dataset
df1['joined_through_referral'].value_counts()

# Check any unknown values or special symbols for categorical features in dataset
df1['preferred_offer_types'].value_counts()

# Check any unknown values or special symbols for categorical features in dataset
# Replace 'Unknown' with NaN values
df1['medium_of_operation'].value_counts()
df1['medium_of_operation'] = df1['medium_of_operation'].replace('?', np.nan)

# Check any unknown values or special symbols for categorical features in dataset
df1['internet_option'].value_counts()

# Check any unknown values or special symbols for categorical features in dataset
df1['used_special_discount'].value_counts()

# Check any unknown values or special symbols for categorical features in dataset
df1['past_complaint'].value_counts()

# Check any unknown values or special symbols for categorical features in dataset
df1['complaint_status'].value_counts()

# Check any unknown values or special symbols for categorical features in dataset
df1['feedback'].value_counts()

df1.columns

# Check any unknown values or special symbols for continuous features in dataset
# Replace 'Unknown' with NaN values
df1['avg_frequency_login_days'].value_counts()
df1['avg_frequency_login_days'] = df1['avg_frequency_login_days'].replace('Error', np.nan)

# Changing the datatype to 'float64'
df1['avg_frequency_login_days'] = df1['avg_frequency_login_days'].astype('float64')

df1.describe().T

# checking for NaN or Null values in the dataest
df1.isnull().sum() / df.shape[0] * 100

# Shape of dataset
df1.shape

# Converting Categorical columns into Nominal using get_dummies()
df2 = pd.get_dummies(df1)
df2.shape

df2.info()

# Check classes in the Dependent variable
df2['churn_risk_score'].value_counts()

# All NaN or Null values are Imputed using Iterative Imputer by taking LinearRegression as base estimator
from sklearn.experimental import enable_iterative_imputer
# now you can import normally from sklearn.impute
from sklearn.impute import IterativeImputer
from sklearn.linear_model import LinearRegression

it = IterativeImputer(estimator=LinearRegression())
newdata = pd.DataFrame(it.fit_transform(df2))
newdata.columns = df2.columns
newdata.head()

# Check null values count
newdata.isnull().sum().sum()

# Camparing statistical summary for dataset before and after Iterative Imputation
newdata.describe()

df2.describe()

newdata.columns

# Modified dataset shape
newdata.shape

# By ingoring '-1' class in dependent variable
newdata = newdata[newdata['churn_risk_score']!= -1]
newdata.shape

# With Pipeline with power train
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PowerTransformer
import lightgbm as lgb

y = newdata['churn_risk_score']
X = newdata.drop('churn_risk_score', axis=1)
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size= .30,random_state=1)
pipe = Pipeline((
("pt",PowerTransformer()),
("lgb", lgb.LGBMClassifier()),
))
pipe.fit(Xtrain,ytrain)
print("Training R2")
print(pipe.score(Xtrain,ytrain))
print("Testing R2")
print(pipe.score(Xtest,ytest))

# By ingoring '-1' class in dependent variable
df3 = df2[df2['churn_risk_score']!= -1]
df3.shape

# With Pipeline with power train
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.experimental import enable_iterative_imputer
from sklearn.preprocessing import PowerTransformer
from sklearn.ensemble import GradientBoostingClassifier

y = df3['churn_risk_score']
X = df3.drop('churn_risk_score', axis=1)
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size= .30,random_state=1)
pipe = Pipeline((
("it",IterativeImputer(estimator=LinearRegression())),
("pt",PowerTransformer()),
("gb", GradientBoostingClassifier()),
))
pipe.fit(Xtrain,ytrain)
print("Training R2")
print(pipe.score(Xtrain,ytrain))
print("Testing R2")
print(pipe.score(Xtest,ytest))

# Copied df2 dataframe as df3
df3 = df2.copy()

# '-1' class is considered as '1' class and model is eveluating
df3['churn_risk_score'].replace(-1,1, inplace=True)
df3['churn_risk_score'].value_counts()

# With Pipeline with power train
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.experimental import enable_iterative_imputer
from sklearn.preprocessing import PowerTransformer
from sklearn.ensemble import GradientBoostingClassifier

y = df3['churn_risk_score']
X = df3.drop('churn_risk_score', axis=1)
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size= .30,random_state=1)
pipe = Pipeline((
("it",IterativeImputer(estimator=LinearRegression())),
("pt",PowerTransformer()),
("gb", GradientBoostingClassifier()),
))
pipe.fit(Xtrain,ytrain)
print("Training R2")
print(pipe.score(Xtrain,ytrain))
print("Testing R2")
print(pipe.score(Xtest,ytest))

# Copied df2 dataframe as df3
df4 = df2.copy()

# '-1' class is considered as '1' class and model is eveluating
df4['churn_risk_score'].replace(-1,5, inplace=True)
df4['churn_risk_score'].value_counts()

# With Pipeline with power train
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PowerTransformer
from sklearn.preprocessing import PolynomialFeatures
from sklearn.decomposition import PCA
import lightgbm as lgb
from sklearn.ensemble import GradientBoostingClassifier

y = df4['churn_risk_score']
X = df4.drop('churn_risk_score', axis=1)
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size= .30,random_state=1)
pipe = Pipeline((
("it",IterativeImputer(estimator=LinearRegression())),
("pt",PowerTransformer()),
("gb", GradientBoostingClassifier()),
))
pipe.fit(Xtrain,ytrain)
print("Training R2")
print(pipe.score(Xtrain,ytrain))
print("Testing R2")
print(pipe.score(Xtest,ytest))

# Tuning the model by making '1' class
# With Pipeline with power train
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PowerTransformer
from sklearn.preprocessing import PolynomialFeatures
from sklearn.decomposition import PCA
import lightgbm as lgb
from sklearn.ensemble import GradientBoostingClassifier

y = df3['churn_risk_score']
X = df3.drop('churn_risk_score', axis=1)
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size= .30,random_state=1)
pipe = Pipeline((
("it",IterativeImputer(estimator=LinearRegression())),
("pt",PowerTransformer()),
("gb", GradientBoostingClassifier(max_features=None, n_estimators=150)), # Changed max_features to None
))
pipe.fit(Xtrain,ytrain)
print("Training R2")
print(pipe.score(Xtrain,ytrain))
print("Testing R2")
print(pipe.score(Xtest,ytest))

test_df = pd.read_csv('test.csv')
test_df.head(2)

test_df.shape

test_df = test_df.drop(['security_no','Name','referral_id','joining_date','last_visit_time','customer_id'], axis=1)
test_df.head(2)

test_df.info()

test_df['feedback'].value_counts()

# Check any unknown values or special symbols for categorical features in dataset
# Replace 'Unknown' with NaN values
test_df['gender'].value_counts()

# In this column no unknown or special symbols are found
test_df['region_category'].value_counts()

# Check any unknown values or special symbols for categorical features in dataset
test_df['membership_category'].value_counts()

# Checking Null values in terms of percentage
test_df.isnull().sum() / test_df.shape[0] * 100

test_df.shape

test_df2 = pd.get_dummies(test_df)
test_df2.shape

test_df2.info()

from sklearn.impute import SimpleImputer

# Use mean imputation as an example
imputer = SimpleImputer(strategy='mean')
test_df3 = pd.DataFrame(imputer.fit_transform(test_df2))
test_df3.columns = test_df2.columns

# Display the first few rows of the imputed DataFrame
test_df3.head()

# Get a list of all columns in the training data (Xtrain)
training_columns = Xtrain.columns.tolist()

# Ensure all columns from the training set are present in the testing set
for col in training_columns:
    if col not in test_df3.columns:
        test_df3[col] = 0  # Add missing columns with a default value of 0

# Ensure only columns from the training set are present in the testing set
test_df3 = test_df3[training_columns]

# Now predict using the updated test_df3
label = pipe.predict(test_df3)
label.shape

# Converting file into .csv for uploading
df_test = pd.read_csv('test.csv')
sample_submission = df_test[['customer_id']]
sample_submission['churn_risk_score'] = label
sample_submission.to_csv('sample_submission.csv')

import numpy as np
import pandas as pd
from sklearn.impute import IterativeImputer
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Step 1: Create a ground truth dataset (or use one without missing values)
np.random.seed(42)
data = pd.DataFrame({
    'Feature1': np.random.rand(100),
    'Feature2': np.random.rand(100),
    'Feature3': np.random.rand(100)
})

# Simulate missing values (e.g., 20% missing values)
data_with_missing = data.copy()
missing_mask = np.random.rand(*data.shape) < 0.2
data_with_missing[missing_mask] = np.nan

# Step 2: Impute missing data
imputer = IterativeImputer(max_iter=10, random_state=42)
imputed_data = pd.DataFrame(imputer.fit_transform(data_with_missing), columns=data.columns)

# Step 3: Calculate metrics
# Only evaluate on positions where data was originally missing
missing_positions = missing_mask
true_values = data[missing_positions]
imputed_values = imputed_data[missing_positions]

# MSE, RMSE, MAE
mse = mean_squared_error(true_values, imputed_values)
rmse = np.sqrt(mse)
mae = mean_absolute_error(true_values, imputed_values)

print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import IterativeImputer
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Step 1: Data Overview
# Original dataset (ground truth)
data = pd.DataFrame({
    'Feature1': np.random.rand(100),
    'Feature2': np.random.rand(100),
    'Feature3': np.random.rand(100)
})

# Simulate missing data
np.random.seed(42)
data_with_missing = data.copy()
missing_mask = np.random.rand(*data.shape) < 0.2
data_with_missing[missing_mask] = np.nan

# Step 2: Impute missing data using IterativeImputer
imputer = IterativeImputer(max_iter=10, random_state=42)
imputed_data = pd.DataFrame(imputer.fit_transform(data_with_missing), columns=data.columns)

# Step 3: Calculate metrics
missing_positions = missing_mask
true_values = data[missing_positions]
imputed_values = imputed_data[missing_positions]

mse = mean_squared_error(true_values, imputed_values)
rmse = np.sqrt(mse)
mae = mean_absolute_error(true_values, imputed_values)

# Step 4: Generate report
report = {
    "Total Missing Values": data_with_missing.isnull().sum().sum(),
    "MSE": mse,
    "RMSE": rmse,
    "MAE": mae
}
report_df = pd.DataFrame(report, index=[0])
print("Report Summary:")
print(report_df)

# Step 5: Visualizations
plt.figure(figsize=(15, 5))

# Missing data visualization
plt.subplot(1, 3, 1)
sns.heatmap(data_with_missing.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Data Pattern")

# Original vs Imputed Data
plt.subplot(1, 3, 2)
sns.kdeplot(data['Feature1'], label='Original', color='blue')
sns.kdeplot(imputed_data['Feature1'], label='Imputed', color='red')
plt.title("Feature 1 Distribution")
plt.legend()

# Error Distribution
plt.subplot(1, 3, 3)
errors = true_values - imputed_values
sns.histplot(errors.values.flatten(), kde=True, color='purple')
plt.title("Imputation Error Distribution")

plt.tight_layout()
plt.show()

import pickle

# Assuming 'pipe' is the trained model pipeline from your previous code
with open('model1.pkl', 'wb') as f:
    pickle.dump(pipe, f) # Changed 'model' to 'pipe'