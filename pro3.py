# -*- coding: utf-8 -*-
"""pro3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yqeRGlnIhOr7t-ggtnU4wXLEYBkGr89x
"""

from google.colab import files

# Upload the file
uploaded = files.upload()

# Access the uploaded file (assuming it's a text file)
for file_name in uploaded.keys():
    with open(file_name, 'r') as file:
        content = file.read()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense, Activation
# from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau
import datetime

data = pd.read_csv('005930.KS_5y.csv')
data.head()

high_prices = data['High'].values
low_prices = data['Low'].values
mid_prices = (high_prices + low_prices) / 2

seq_len = 50
sequence_length = seq_len + 1

result = []
for index in range(len(mid_prices) - sequence_length):
    result.append(mid_prices[index: index + sequence_length])

normalized_data = []
for window in result:
    normalized_window = [((float(p) / float(window[0])) - 1) for p in window]
    normalized_data.append(normalized_window)

result = np.array(normalized_data)

# split train and test data
row = int(round(result.shape[0] * 0.9))
train = result[:row, :]
np.random.shuffle(train)

x_train = train[:, :-1]
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
y_train = train[:, -1]

x_test = result[row:, :-1]
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))
y_test = result[row:, -1]

x_train.shape, x_test.shape

model = Sequential()

model.add(LSTM(50, return_sequences=True, input_shape=(50, 1)))

model.add(LSTM(64, return_sequences=False))

model.add(Dense(1, activation='linear'))

model.compile(loss='mse', optimizer='rmsprop')

model.summary()

model.fit(x_train, y_train,
    validation_data=(x_test, y_test),
    batch_size=10,
    epochs=20)

pred = model.predict(x_test)

fig = plt.figure(facecolor='white', figsize=(20, 10))
ax = fig.add_subplot(111)
ax.plot(y_test, label='True')
ax.plot(pred, label='Prediction')
ax.legend()
plt.show()

import numpy as np
import pandas as pd
# Explicitly enable IterativeImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Step 1: Create a ground truth dataset (or use one without missing values)
np.random.seed(42)
data = pd.DataFrame({
    'Feature1': np.random.rand(100),
    'Feature2': np.random.rand(100),
    'Feature3': np.random.rand(100)
})

# Simulate missing values (e.g., 20% missing values)
data_with_missing = data.copy()
missing_mask = np.random.rand(*data.shape) < 0.2
data_with_missing[missing_mask] = np.nan

# Step 2: Impute missing data
imputer = IterativeImputer(max_iter=10, random_state=42)
imputed_data = pd.DataFrame(imputer.fit_transform(data_with_missing), columns=data.columns)

# Step 3: Calculate metrics
# Only evaluate on positions where data was originally missing
missing_positions = missing_mask
true_values = data[missing_positions]
imputed_values = imputed_data[missing_positions]

# MSE, RMSE, MAE
mse = mean_squared_error(true_values, imputed_values)
rmse = np.sqrt(mse)
mae = mean_absolute_error(true_values, imputed_values)

print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")

import pickle

# Save the model
with open('model3.pkl', 'wb') as f:
    pickle.dump(model, f)